use byte_unit::{Byte, UnitType};
use clap::{Parser, ValueEnum};
use indicatif::{ProgressBar, ProgressStyle};
use rand::{distributions::Alphanumeric, prelude::*, rngs::StdRng, SeedableRng};
use redis::AsyncCommands;
use rkyv::{
    from_bytes, rancor::Error as RkyvError, to_bytes, Archive, Deserialize as RkyvDeserialize,
    Serialize as RkyvSerialize,
};
use rkyv::Deserialize;
// use serde::{Deserialize, Serialize};
use serde::{Deserialize as SerdeDeserialize, Serialize};
use std::sync::Arc;
use std::time::{Duration, Instant, SystemTime, UNIX_EPOCH};
use zstd::stream::{decode_all, encode_all};

use bitcode;
// Import the Message trait from prost, which provides encode/decode methods.
use prost::Message;

// This module block includes the Rust code generated by `prost-build` from our .proto file.
// The file path is determined by Cargo's `OUT_DIR` environment variable.
pub mod pb {
    include!(concat!(env!("OUT_DIR"), "/benchmark.rs"));
}

const DEFAULT_KEY_PREFIX: &str = "chat";
const DEFAULT_BATCH_SIZE: usize = 50;
const PROGRESS_UPDATE_INTERVAL: usize = 1000;

#[derive(ValueEnum, Clone, Debug, Copy, PartialEq, Eq)]
enum DataFormat {
    String,
    Json,
    Bitcode,
    Protobuf,
    Rkyv,
}

#[derive(Parser, Debug, Clone)]
#[clap(author, version, about, long_about = None)]
struct Cli {
    #[clap(short, long, default_value = "redis://127.0.0.1:6379")]
    redis_url: String,

    #[clap(short, long, default_value = "redis://127.0.0.1:6380")]
    valkey_url: String,

    #[clap(short = 'o', long, default_value_t = 100_000, help="Total number of operations to perform.")]
    num_ops: usize,

    #[clap(short, long, default_value_t = 50, help="Number of concurrent client threads.")]
    concurrency: usize,

    #[clap(long, default_value_t = 128, help="Base size in bytes for the data part of a message payload.")]
    value_size: usize,

    #[clap(long, value_enum, default_value_t = DataFormat::String, help = "Data serialization format for values")]
    format: DataFormat,

    #[clap(long, default_value_t = 1000, help="Number of simulated chat rooms.")]
    num_chats: usize,

    #[clap(long, default_value_t = 100, help="Number of messages to keep in chat history (for LTRIM).")]
    history_len: usize,

    #[clap(long, default_value_t = 50, help="Number of messages to fetch in a single READ operation (for LRANGE).")]
    read_size: usize,

    #[clap(long, help = "Run only write benchmarks")]
    write_only: bool,

    #[clap(long, help = "Run only read benchmarks")]
    read_only: bool,

    #[clap(long, help = "Use explicit pipelining for commands")]
    pipeline: bool,

    #[clap(long, default_value_t = DEFAULT_BATCH_SIZE, help = "Batch size for pipelined commands")]
    batch_size: usize,

    #[clap(long, help = "Skip latency tracking (useful for max throughput tests or pipelined mode)")]
    no_latency: bool,

    #[clap(long, help = "Simulate zero-copy reads by skipping/optimizing deserialization")]
    zero_copy_read: bool,

    #[clap(long, help = "Enable zstd compression for values")]
    compress_zstd: bool,
}

// #[derive(Serialize, Deserialize, Archive, RkyvDeserialize, RkyvSerialize, Debug, Clone)]
#[derive(Serialize, SerdeDeserialize, Archive, RkyvDeserialize, RkyvSerialize, Debug, Clone)]
struct BenchmarkPayload {
    data: String,
    timestamp: u64,
    id: usize,
}

// --- Data Generation ---
fn generate_random_string(len: usize, rng: &mut impl Rng) -> String {
    let mut s = String::with_capacity(len);
    for _ in 0..len {
        let item_from_sample: u8 = Alphanumeric.sample(rng);
        s.push(item_from_sample as char);
    }
    s
}

struct PreGeneratedData {
    keys: Arc<Vec<String>>,
    values: Option<Arc<Vec<Vec<u8>>>>,
}

impl PreGeneratedData {
    fn new(cli: &Cli, op_type_for_values: &str) -> Self {
        let mut rng = rand::thread_rng();
        println!(
            "Pre-generating {} chat room keys and potentially {} message values (base size {}B) using {:?} format...",
            cli.num_chats, if op_type_for_values == "WRITE" { cli.num_ops } else { 0 }, cli.value_size, cli.format
        );

        let keys = Arc::new(
            (0..cli.num_chats)
                .map(|i| format!("{}:{}", DEFAULT_KEY_PREFIX, i))
                .collect::<Vec<_>>(),
        );

        let values = if op_type_for_values == "WRITE" {
            let values_bytes: Vec<Vec<u8>> = (0..cli.num_ops).map(|i| {
                let random_data = generate_random_string(cli.value_size, &mut rng);
                let timestamp = SystemTime::now().duration_since(UNIX_EPOCH).unwrap().as_secs();

                let serialized_bytes = match cli.format {
                    DataFormat::String => random_data.into_bytes(),
                    DataFormat::Json => {
                        let payload = BenchmarkPayload { data: random_data, timestamp, id: i };
                        serde_json::to_vec(&payload).unwrap()
                    },
                    DataFormat::Bitcode => {
                        let payload = BenchmarkPayload { data: random_data, timestamp, id: i };
                        bitcode::serialize(&payload).unwrap()
                    },
                    DataFormat::Protobuf => {
                        let payload = pb::BenchmarkPayload {
                            data: random_data,
                            timestamp,
                            id: i as u64,
                        };
                        payload.encode_to_vec()
                    },
                    DataFormat::Rkyv => {
                        let payload = BenchmarkPayload { data: random_data, timestamp, id: i };
                        to_bytes::<RkyvError>(&payload).unwrap().into_vec()
                    }
                };

                if cli.compress_zstd {
                    // Using compression level 0 is a default that's fast.
                    encode_all(serialized_bytes.as_slice(), 0).unwrap()
                } else {
                    serialized_bytes
                }
            }).collect();

            if !values_bytes.is_empty() {
                 let total_bytes: usize = values_bytes.iter().map(|v| v.len()).sum();
                 let avg_size = total_bytes as f64 / values_bytes.len() as f64;
                 let compressed_str = if cli.compress_zstd { " (compressed)" } else { "" };
                 println!("Pre-generated {} values. Average serialized value size{}: {:.2} B.", values_bytes.len(), compressed_str, avg_size);
            }
            Some(Arc::new(values_bytes))
        } else {
            None
        };
        println!("Data generation complete.");
        Self { keys, values }
    }
}

struct BenchResult {
    ops_per_second: f64,
    total_time: Duration,
    avg_latency_ms: Option<f64>,
    p99_latency_ms: Option<f64>,
    errors: usize,
    total_bytes_written: u64,
    // CORRECTED: Typo u664 -> u64
    total_bytes_read: u64,
}

struct WorkerResult {
    histogram: Option<hdrhistogram::Histogram<u64>>,
    errors: usize,
    ops_done: usize,
    bytes_written: u64,
    bytes_read: u64,
}

async fn run_benchmark(
    db_name: &str,
    op_type_ref: &str,
    db_url_slice: &str,
    cli: &Cli,
    data: &PreGeneratedData,
    track_latency: bool,
) -> Result<BenchResult, Box<dyn std::error::Error>> {
    let op_type_owned = op_type_ref.to_string();

    let mode_str = if cli.pipeline { "PIPELINED" } else { "INDIVIDUAL" };
    let read_mode_str = if op_type_owned == "READ" && cli.zero_copy_read { " (Zero-Copy)" } else { "" };
    println!(
        "\nBenchmarking {} {} (Chat Sim){}{} ({:?}{} format, {} ops, {} concurrent, {} mode)...",
        db_name,
        op_type_owned,
        read_mode_str,
        if op_type_owned == "READ" { format!(" READ_SIZE: {}", cli.read_size) } else { "".to_string() },
        cli.format,
        if cli.compress_zstd { "+zstd" } else { "" },
        cli.num_ops,
        cli.concurrency,
        mode_str,
    );
    
    let client = redis::Client::open(db_url_slice)?;
    let mut connections = Vec::with_capacity(cli.concurrency);
    for _ in 0..cli.concurrency {
        connections.push(client.get_multiplexed_async_connection().await?);
    }
    let connections = Arc::new(connections);

    let pb = ProgressBar::new(cli.num_ops as u64);
    pb.set_style(
        ProgressStyle::default_bar()
            .template("{spinner:.green} [{elapsed_precise}] [{bar:40.cyan/blue}] {pos}/{len} ({eta}) {msg}")
            .expect("Failed to create progress bar style")
            .progress_chars("#>-"),
    );
    pb.set_message(format!("{} {}{}", db_name, op_type_owned, read_mode_str));
    let pb_arc = Arc::new(pb);

    let start_time = Instant::now();
    let mut tasks = Vec::with_capacity(cli.concurrency);
    let ops_per_worker = (cli.num_ops + cli.concurrency - 1) / cli.concurrency;

    for worker_id in 0..cli.concurrency {
        let mut conn = connections[worker_id % connections.len()].clone();
        let keys_ref = Arc::clone(&data.keys);
        let values_ref = data.values.as_ref().map(Arc::clone);
        let progress_bar_clone = Arc::clone(&pb_arc);
        let task_op_type = op_type_owned.clone();
        
        let task_format = cli.format;
        let task_zero_copy_read = cli.zero_copy_read;
        let use_pipeline = cli.pipeline;
        let batch_size = cli.batch_size;
        let history_len = cli.history_len;
        let read_size = cli.read_size;
        let use_compression = cli.compress_zstd;

        let worker_start_idx = worker_id * ops_per_worker;
        let worker_end_idx = ((worker_id + 1) * ops_per_worker).min(cli.num_ops);

        if worker_start_idx >= worker_end_idx { continue; }

        tasks.push(tokio::spawn(async move {
            let mut hist = if track_latency && !use_pipeline {
                Some(hdrhistogram::Histogram::<u64>::new(3).unwrap())
            } else { None };
            let mut local_errors: usize = 0;
            let mut local_ops_done: usize = 0;
            let mut local_bytes_written: u64 = 0;
            let mut local_bytes_read: u64 = 0;
            let num_worker_ops = worker_end_idx - worker_start_idx;
            let mut worker_progress_counter: u64 = 0;
            let mut rng = StdRng::from_entropy();

            if use_pipeline {
                for chunk_start_offset in (0..num_worker_ops).step_by(batch_size) {
                    let current_batch_size = (chunk_start_offset + batch_size).min(num_worker_ops) - chunk_start_offset;
                    if current_batch_size == 0 { continue; }
                    
                    if task_op_type == "WRITE" {
                        let mut pipe = redis::pipe();
                        for i in 0..current_batch_size {
                            let op_idx = worker_start_idx + chunk_start_offset + i;
                            let key = &keys_ref[rng.gen_range(0..keys_ref.len())];
                            let value = &values_ref.as_ref().unwrap()[op_idx];
                            local_bytes_written += value.len() as u64;
                            pipe.lpush(key, value).ltrim(key, 0, history_len as isize - 1).ignore();
                        }
                        match pipe.query_async::<()>(&mut conn).await {
                            Ok(_) => { local_ops_done += current_batch_size; }
                            Err(_) => { local_errors += current_batch_size; }
                        }
                    } else { // READ
                        let mut pipe = redis::pipe();
                        for _ in 0..current_batch_size {
                            let key = &keys_ref[rng.gen_range(0..keys_ref.len())];
                            pipe.lrange(key, 0, read_size as isize - 1);
                        }
                        match pipe.query_async::<Vec<Vec<Vec<u8>>>>(&mut conn).await {
                            Ok(results) => {
                                local_bytes_read += results
                                    .iter()
                                    .flat_map(|messages| messages.iter())
                                    .map(|message_bytes| message_bytes.len() as u64)
                                    .sum::<u64>();

                                for messages in results {
                                    if !process_read_result(messages, task_format, task_zero_copy_read, use_compression) {
                                        local_errors += 1;
                                    }
                                }
                                local_ops_done += current_batch_size;
                            },
                            Err(_) => { local_errors += current_batch_size; }
                        }
                    }

                    worker_progress_counter += current_batch_size as u64;
                    if worker_progress_counter >= PROGRESS_UPDATE_INTERVAL as u64 {
                        progress_bar_clone.inc(worker_progress_counter);
                        worker_progress_counter = 0;
                    }
                }
            } else { // Individual commands
                for i in 0..num_worker_ops {
                    let op_idx = worker_start_idx + i;
                    let key = &keys_ref[rng.gen_range(0..keys_ref.len())];
                    let op_start_time = if track_latency { Some(Instant::now()) } else { None };
                    
                    let op_successful = if task_op_type == "WRITE" {
                        let value = &values_ref.as_ref().unwrap()[op_idx];
                        local_bytes_written += value.len() as u64;
                        let mut pipe = redis::pipe();
                        pipe.lpush(key, value).ltrim(key, 0, history_len as isize - 1).ignore();
                        pipe.query_async::<()>(&mut conn).await.is_ok()
                    } else { // READ operation
                        match conn.lrange::<&str, Vec<Vec<u8>>>(key, 0, read_size as isize - 1).await {
                            Ok(messages) => {
                                local_bytes_read += messages.iter().map(|m: &Vec<u8>| m.len() as u64).sum::<u64>();
                                process_read_result(messages, task_format, task_zero_copy_read, use_compression)
                            },
                            Err(_) => false
                        }
                    };

                    if let Some(st) = op_start_time {
                        if let Some(h) = hist.as_mut() { let _ = h.record(st.elapsed().as_micros() as u64); }
                    }

                    if !op_successful { local_errors += 1; }
                    local_ops_done += 1;

                    worker_progress_counter += 1;
                    if worker_progress_counter >= PROGRESS_UPDATE_INTERVAL as u64 {
                        progress_bar_clone.inc(worker_progress_counter);
                        worker_progress_counter = 0;
                    }
                }
            }
            if worker_progress_counter > 0 {
                progress_bar_clone.inc(worker_progress_counter);
            }
            WorkerResult { 
                histogram: hist, 
                errors: local_errors, 
                ops_done: local_ops_done,
                bytes_written: local_bytes_written,
                bytes_read: local_bytes_read,
            }
        }));
    }

    let worker_results = futures::future::join_all(tasks).await;
    let total_time = start_time.elapsed();
    
    if pb_arc.position() < cli.num_ops as u64 && cli.num_ops > 0 {
        pb_arc.set_position(cli.num_ops as u64);
    }
    pb_arc.finish_with_message(format!("{} {} {} Done", db_name, op_type_owned, mode_str));

    let mut final_histogram = if track_latency && !cli.pipeline {
        Some(hdrhistogram::Histogram::<u64>::new(3).unwrap())
    } else { None };
    let mut total_errors = 0;
    let mut total_bytes_written: u64 = 0;
    let mut total_bytes_read: u64 = 0;

    for result in worker_results {
        match result {
            Ok(wr) => {
                if let (Some(fh), Some(wh)) = (final_histogram.as_mut(), wr.histogram.as_ref()) {
                    let _ = fh.add(wh);
                }
                total_errors += wr.errors;
                total_bytes_written += wr.bytes_written;
                total_bytes_read += wr.bytes_read;
            }
            Err(e) => {
                eprintln!("Worker task panicked: {}", e);
                total_errors += cli.num_ops / cli.concurrency.max(1);
            }
        }
    }

    if total_errors > 0 {
        println!("WARNING: {}/{} operations reported errors for {} {}", total_errors, cli.num_ops, db_name, op_type_owned);
    }

    let successful_ops = cli.num_ops.saturating_sub(total_errors);
    let ops_per_second = if total_time.as_secs_f64() > 0.0 {
        successful_ops as f64 / total_time.as_secs_f64()
    } else { if successful_ops > 0 { f64::INFINITY } else { 0.0 } };

    let (avg_lat, p99_lat) = if let Some(hist) = final_histogram.as_ref() {
        if hist.len() > 0 { (Some(hist.mean() as f64 / 1000.0), Some(hist.value_at_percentile(99.0) as f64 / 1000.0)) }
        else { (Some(0.0), Some(0.0)) }
    } else { (None, None) };

    Ok(BenchResult {
        ops_per_second,
        total_time,
        avg_latency_ms: avg_lat,
        p99_latency_ms: p99_lat,
        errors: total_errors,
        total_bytes_written,
        total_bytes_read,
    })
}

fn process_read_result(messages: Vec<Vec<u8>>, format: DataFormat, zero_copy: bool, compressed: bool) -> bool {
    if messages.is_empty() { return true; }

    for bytes in &messages {
        // Decompress first if needed. This is now a required step for all reads if compression is enabled.
        let decompressed_result = if compressed {
            decode_all(bytes.as_slice())
        } else {
            Ok(bytes.to_vec()) // Wrap in Ok for consistent matching
        };

        let owned_bytes = match decompressed_result {
            Ok(b) => b,
            Err(_) => return false, // Decompression failed
        };

        if zero_copy {
            // "Zero-copy" in a compressed world means we decompress but avoid full deserialization.
            // We can still do cheap checks on the decompressed data.
            let ok = if matches!(format, DataFormat::Rkyv) {
                if let Ok(archived) = from_bytes::<BenchmarkPayload, RkyvError>(&owned_bytes) {
                    let _ = archived.id; // Simulate field access
                    true
                } else {
                    false
                }
            } else {
                // For other formats, with zero_copy on, we just decompress and move on.
                true
            };
            if !ok { return false; }
        } else {
            // Full deserialization path on decompressed data.
            let ok = match format {
                DataFormat::Json => serde_json::from_slice::<BenchmarkPayload>(&owned_bytes).is_ok(),
                DataFormat::Bitcode => bitcode::deserialize::<BenchmarkPayload>(&owned_bytes).is_ok(),
                DataFormat::String => String::from_utf8(owned_bytes).is_ok(),
                DataFormat::Protobuf => pb::BenchmarkPayload::decode(owned_bytes.as_slice()).is_ok(),
                DataFormat::Rkyv => {
                    match from_bytes::<BenchmarkPayload, RkyvError>(&owned_bytes) {
                        Ok(archived) => {
                            // For full deserialization, we can use the archived value directly
                            // or just access a field to simulate usage
                            let _ = &archived.data;
                            let _ = archived.timestamp;
                            let _ = archived.id;
                            true
                        },
                        Err(_) => false
                    }
                },
            };
            if !ok { return false; }
        }
    }
    true
}


#[tokio::main]
async fn main() -> Result<(), Box<dyn std::error::Error>> {
    let cli = Cli::parse();
    println!("Configuration: Ops={}, Concurrency={}, Format={:?}, ValueBaseSize={}B", cli.num_ops, cli.concurrency, cli.format, cli.value_size);
    println!("Chat Simulation: Chats={}, HistoryLen={}, ReadSize={}", cli.num_chats, cli.history_len, cli.read_size);
    println!("Technical: Pipeline={}, BatchSize={}, NoLatency={}, ZeroCopyRead={}, Compression={}", cli.pipeline, cli.batch_size, cli.no_latency, cli.zero_copy_read, if cli.compress_zstd {"zstd"} else {"none"});
    println!("Ensure Redis is running on {} and Valkey on {}", cli.redis_url, cli.valkey_url);

    if cli.num_ops == 0 { println!("Number of operations is 0, exiting."); return Ok(()); }

    let benchmarks_to_run = match (cli.write_only, cli.read_only) {
        (true, false) => vec!["WRITE"],
        (false, true) => vec!["read"],
        (false, false) => vec!["WRITE", "read"],
        (true, true) => { println!("Cannot specify both --write-only and --read-only."); return Ok(()); }
    };

    let track_latency_globally = !cli.no_latency;

    let write_data = PreGeneratedData::new(&cli, "WRITE");
    let read_data = PreGeneratedData { keys: Arc::clone(&write_data.keys), values: None };

    if benchmarks_to_run.contains(&"read") {
        let mut prepop_cli = cli.clone();
        prepop_cli.pipeline = true;
        prepop_cli.batch_size = cli.batch_size.max(200);

        println!("\nINFO: Pre-populating Redis for READ benchmark...");
        run_benchmark("Redis-PrePop", "WRITE", &cli.redis_url, &prepop_cli, &write_data, false).await?;
        println!("\nINFO: Pre-populating Valkey for READ benchmark...");
        run_benchmark("Valkey-PrePop", "WRITE", &cli.valkey_url, &prepop_cli, &write_data, false).await?;
    }

    let mut results_table = Vec::new();
    for db_type in &["Redis", "Valkey"] {
        let url_string = if *db_type == "Redis" { &cli.redis_url } else { &cli.valkey_url };
        for op_type_str_ref in &benchmarks_to_run {
            let current_data = if *op_type_str_ref == "WRITE" { &write_data } else { &read_data };
            let actual_track_latency = track_latency_globally && !cli.pipeline;

            match run_benchmark(db_type, op_type_str_ref, url_string.as_str(), &cli, current_data, actual_track_latency).await {
                Ok(result) => {
                     results_table.push((
                        *db_type,
                        *op_type_str_ref,
                        result.ops_per_second,
                        result.total_time.as_secs_f64(),
                        result.avg_latency_ms,
                        result.p99_latency_ms,
                        result.errors,
                        result.total_bytes_written,
                        result.total_bytes_read,
                    ));
                },
                Err(e) => {
                    eprintln!("Error benchmarking {} {}: {}", db_type, op_type_str_ref, e);
                     match redis::Client::open(url_string.as_str()) {
                        Ok(client_ping) => {
                            match client_ping.get_multiplexed_async_connection().await {
                                Ok(mut conn_ping) => {
                                    match redis::cmd("PING").query_async::<String>(&mut conn_ping).await {
                                        Ok(pong) if pong == "PONG" => println!("{} is responsive (PING successful).", db_type),
                                        _ => println!("{} is NOT responsive or PING failed.", db_type),
                                    }
                                }
                                Err(ce) => println!("Could not connect to {} for PING: {}", db_type, ce),
                            }
                        }
                        Err(oe) => println!("Could not open client for {} for PING: {}", db_type, oe),
                    }
                }
            }
        }
    }

    println!("\n--- Benchmark Summary (Format: {:?}{}, Workload: Chat Simulation) ---", cli.format, if cli.compress_zstd { "+zstd" } else { "" });
    let show_latency_in_table = track_latency_globally && !cli.pipeline;
    if show_latency_in_table {
        println!("{:<12} | {:<18} | {:<14} | {:<14} | {:<14} | {:<12} | {:<12} | {:<8}", "Database", "Op Type", "Ops/sec", "Speed (MB/s)", "Total Traffic", "Avg Lat(ms)", "P99 Lat(ms)", "Errors");
        println!("{:-<13}|{:-<20}|{:-<16}|{:-<16}|{:-<16}|{:-<14}|{:-<14}|{:-<10}", "", "", "", "", "", "", "", "");
    } else {
        println!("{:<12} | {:<18} | {:<14} | {:<14} | {:<14} | {:<8}", "Database", "Op Type", "Ops/sec", "Speed (MB/s)", "Total Traffic", "Errors");
        println!("{:-<13}|{:-<20}|{:-<16}|{:-<16}|{:-<16}|{:-<10}", "", "", "", "", "", "");
    }

    for (db, op, ops_sec, time_s, avg_lat, p99_lat, errors, bytes_written, bytes_read) in results_table.iter() {
        let op_name = if *op == "read" {
            if cli.zero_copy_read { "READ (Chat, ZC)".to_string() } else { "READ (Chat)".to_string() }
        } else {
            "WRITE (Chat)".to_string()
        };

        let total_traffic_bytes = *bytes_written + *bytes_read;
        let traffic_speed_mb_s = if *time_s > 0.0 {
            (total_traffic_bytes as f64 / *time_s) / 1_000_000.0
        } else { 0.0 };
        let total_traffic_str = Byte::from(total_traffic_bytes).get_appropriate_unit(UnitType::Binary).to_string();

        if show_latency_in_table {
            println!("{:<12} | {:<18} | {:<14.2} | {:<14.2} | {:<14} | {:<12.3} | {:<12.3} | {:<8}", db, op_name, ops_sec, traffic_speed_mb_s, total_traffic_str, avg_lat.unwrap_or(0.0), p99_lat.unwrap_or(0.0), errors);
        } else {
            println!("{:<12} | {:<18} | {:<14.2} | {:<14.2} | {:<14} | {:<8}", db, op_name, ops_sec, traffic_speed_mb_s, total_traffic_str, errors);
        }
    }

    Ok(())
}