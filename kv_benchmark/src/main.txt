use clap::{Parser, ValueEnum};
use indicatif::{ProgressBar, ProgressStyle};
use rand::{
    Rng,
    distributions::{Alphanumeric, Distribution}
};
use redis::AsyncCommands;
use serde::{Deserialize, Serialize};
use std::sync::Arc;
use std::time::{Duration, Instant, SystemTime, UNIX_EPOCH};

use bitcode;
// Import the Message trait from prost, which provides encode/decode methods.
use prost::Message;

// This module block includes the Rust code generated by `prost-build` from our .proto file.
// The file path is determined by Cargo's `OUT_DIR` environment variable.
pub mod pb {
    include!(concat!(env!("OUT_DIR"), "/benchmark.rs"));
}

const DEFAULT_KEY_PREFIX: &str = "benchkey";
const DEFAULT_BATCH_SIZE: usize = 50;
const PROGRESS_UPDATE_INTERVAL: usize = 1000;

#[derive(ValueEnum, Clone, Debug, Copy)]
enum DataFormat {
    String,
    Json,
    Bitcode,
    Protobuf, // New format option
}

#[derive(Parser, Debug)]
#[clap(author, version, about, long_about = None)]
struct Cli {
    #[clap(short, long, default_value = "redis://127.0.0.1:6379")]
    redis_url: String,

    #[clap(short, long, default_value = "redis://127.0.0.1:6380")]
    valkey_url: String,

    #[clap(short, long, default_value_t = 100_000)]
    num_ops: usize,

    #[clap(short, long, default_value_t = 50)]
    concurrency: usize,

    #[clap(long, default_value_t = 16)]
    key_size: usize,

    #[clap(long, default_value_t = 128)]
    value_size: usize,

    #[clap(long, value_enum, default_value_t = DataFormat::String, help = "Data serialization format for values")]
    format: DataFormat,

    #[clap(long, help = "Run only write benchmarks")]
    write_only: bool,

    #[clap(long, help = "Run only read benchmarks")]
    read_only: bool,

    #[clap(long, help = "Use explicit pipelining for commands")]
    pipeline: bool,

    #[clap(long, default_value_t = DEFAULT_BATCH_SIZE, help = "Batch size for pipelined commands")]
    batch_size: usize,

    #[clap(long, help = "Skip latency tracking (useful for max throughput tests or pipelined mode)")]
    no_latency: bool,

    #[clap(long, help = "Simulate zero-copy reads by skipping deserialization")]
    zero_copy_read: bool,
}

// We still keep the serde-based struct for JSON/Bitcode. Protobuf uses its own generated struct.
#[derive(Serialize, Deserialize, Debug, Clone)]
struct BenchmarkPayload {
    data: String,
    timestamp: u64,
    id: usize,
}

// --- Data Generation ---
fn generate_random_string(len: usize, rng: &mut impl Rng) -> String {
    let mut s = String::with_capacity(len);
    for _ in 0..len {
        let item_from_sample: u8 = Alphanumeric.sample(rng);
        s.push(item_from_sample as char);
    }
    s
}

struct PreGeneratedData {
    keys: Arc<Vec<String>>,
    values: Option<Arc<Vec<Vec<u8>>>>,
}

impl PreGeneratedData {
    fn new(num_ops: usize, key_size: usize, value_size: usize, op_type_for_values: &str, format: DataFormat) -> Self {
        let mut rng = rand::thread_rng();
        println!(
            "Pre-generating {} keys ({}B) and potentially values (base size {}B) using {:?} format...",
            num_ops, key_size, if op_type_for_values == "WRITE" { value_size } else { 0 }, format
        );

        let keys = Arc::new(
            (0..num_ops)
                .map(|i| {
                    let random_suffix_len = key_size.saturating_sub(DEFAULT_KEY_PREFIX.len() + 6);
                    let random_suffix = generate_random_string(random_suffix_len, &mut rng);
                    format!("{}:{:05}:{}", DEFAULT_KEY_PREFIX, i % (num_ops / 10).max(1), random_suffix)
                        .chars()
                        .take(key_size)
                        .collect::<String>()
                })
                .collect::<Vec<_>>(),
        );

        let values = if op_type_for_values == "WRITE" {
            let values_bytes: Vec<Vec<u8>> = (0..num_ops).map(|i| {
                let random_data = generate_random_string(value_size, &mut rng);
                let timestamp = SystemTime::now().duration_since(UNIX_EPOCH).unwrap().as_secs();

                match format {
                    DataFormat::String => random_data.into_bytes(),
                    DataFormat::Json => {
                        let payload = BenchmarkPayload { data: random_data, timestamp, id: i };
                        serde_json::to_vec(&payload).unwrap()
                    },
                    DataFormat::Bitcode => {
                        let payload = BenchmarkPayload { data: random_data, timestamp, id: i };
                        bitcode::serialize(&payload).unwrap()
                    },
                    DataFormat::Protobuf => {
                        let payload = pb::BenchmarkPayload {
                            data: random_data,
                            timestamp,
                            id: i as u64,
                        };
                        payload.encode_to_vec()
                    },
                }
            }).collect();

            if !values_bytes.is_empty() {
                 let total_bytes: usize = values_bytes.iter().map(|v| v.len()).sum();
                 let avg_size = total_bytes as f64 / values_bytes.len() as f64;
                 println!("Pre-generated {} values. Average serialized value size: {:.2} B.", values_bytes.len(), avg_size);
            }
            Some(Arc::new(values_bytes))
        } else {
            None
        };
        println!("Data generation complete.");
        Self { keys, values }
    }
}

struct BenchResult {
    ops_per_second: f64,
    total_time: Duration,
    avg_latency_ms: Option<f64>,
    p99_latency_ms: Option<f64>,
    errors: usize,
}

struct WorkerResult {
    histogram: Option<hdrhistogram::Histogram<u64>>,
    errors: usize,
    ops_done: usize,
}

async fn run_benchmark(
    db_name: &str,
    op_type_ref: &str,
    db_url_slice: &str,
    num_ops: usize,
    concurrency: usize,
    data: &PreGeneratedData,
    use_pipeline: bool,
    batch_size: usize,
    track_latency: bool,
    format: DataFormat,
    zero_copy_read: bool,
) -> Result<BenchResult, Box<dyn std::error::Error>> {
    let op_type_owned = op_type_ref.to_string();

    let mode_str = if use_pipeline { "PIPELINED" } else { "INDIVIDUAL" };
    let read_mode_str = if op_type_owned == "READ" && zero_copy_read { " (Raw)" } else { "" };
    println!(
        "\nBenchmarking {} {}{} ({:?} format, {} ops, {} concurrent, {} mode)...",
        db_name,
        op_type_owned,
        read_mode_str,
        format,
        num_ops,
        concurrency,
        mode_str,
    );
    
    let client = redis::Client::open(db_url_slice)?;
    let mut connections = Vec::with_capacity(concurrency);
    for _ in 0..concurrency {
        connections.push(client.get_multiplexed_async_connection().await?);
    }
    let connections = Arc::new(connections);

    let pb = ProgressBar::new(num_ops as u64);
    pb.set_style(
        ProgressStyle::default_bar()
            .template("{spinner:.green} [{elapsed_precise}] [{bar:40.cyan/blue}] {pos}/{len} ({eta}) {msg}")
            .expect("Failed to create progress bar style")
            .progress_chars("#>-"),
    );
    pb.set_message(format!("{} {}{}", db_name, op_type_owned, read_mode_str));
    let pb_arc = Arc::new(pb);

    let start_time = Instant::now();
    let mut tasks = Vec::with_capacity(concurrency);
    let ops_per_worker = (num_ops + concurrency - 1) / concurrency;

    for worker_id in 0..concurrency {
        let mut conn = connections[worker_id % connections.len()].clone();
        let keys_ref = Arc::clone(&data.keys);
        let values_ref = data.values.as_ref().map(Arc::clone);
        let progress_bar_clone = Arc::clone(&pb_arc);
        let task_op_type = op_type_owned.clone();
        let task_data_format = format;
        let task_zero_copy_read = zero_copy_read;

        let worker_start_idx = worker_id * ops_per_worker;
        let worker_end_idx = ((worker_id + 1) * ops_per_worker).min(num_ops);

        if worker_start_idx >= worker_end_idx { continue; }

        tasks.push(tokio::spawn(async move {
            let mut hist = if track_latency && !use_pipeline {
                Some(hdrhistogram::Histogram::<u64>::new(3).unwrap())
            } else { None };
            let mut local_errors: usize = 0;
            let mut local_ops_done: usize = 0;
            let num_worker_ops = worker_end_idx - worker_start_idx;
            let mut worker_progress_counter: u64 = 0;

            if use_pipeline {
                for chunk_start_offset in (0..num_worker_ops).step_by(batch_size) {
                    let current_batch_size = (chunk_start_offset + batch_size).min(num_worker_ops) - chunk_start_offset;
                    if current_batch_size == 0 { continue; }
                    let mut pipe = redis::pipe();
                    for i in 0..current_batch_size {
                        let op_idx = worker_start_idx + chunk_start_offset + i;
                        let key = &keys_ref[op_idx];
                        if task_op_type == "WRITE" {
                            let value = &values_ref.as_ref().unwrap()[op_idx];
                            pipe.cmd("SET").arg(key).arg(value).ignore();
                        } else {
                            pipe.cmd("GET").arg(key).ignore();
                        }
                    }
                    match pipe.query_async::<()>(&mut conn).await {
                        Ok(_) => { local_ops_done += current_batch_size; }
                        Err(_) => { local_errors += current_batch_size; local_ops_done += current_batch_size; }
                    }
                    worker_progress_counter += current_batch_size as u64;
                    if worker_progress_counter >= PROGRESS_UPDATE_INTERVAL as u64 {
                        progress_bar_clone.inc(worker_progress_counter);
                        worker_progress_counter = 0;
                    }
                }
            } else { // Individual commands
                for i in 0..num_worker_ops {
                    let op_idx = worker_start_idx + i;
                    let key = &keys_ref[op_idx];
                    let op_start_time = if track_latency { Some(Instant::now()) } else { None };
                    
                    let op_successful = if task_op_type == "WRITE" {
                        let value = &values_ref.as_ref().unwrap()[op_idx];
                        conn.set::<_, _, ()>(key, value).await.is_ok()
                    } else { // READ operation
                        if task_zero_copy_read {
                            conn.get::<_, Option<Vec<u8>>>(key).await
                                .map(|v| v.is_some())
                                .unwrap_or(false)
                        } else {
                            let res: redis::RedisResult<Option<Vec<u8>>> = conn.get(key).await;
                            match res {
                                Ok(Some(bytes)) => {
                                    match task_data_format {
                                        DataFormat::Json => serde_json::from_slice::<BenchmarkPayload>(&bytes).is_ok(),
                                        DataFormat::Bitcode => bitcode::deserialize::<BenchmarkPayload>(&bytes).is_ok(),
                                        DataFormat::String => String::from_utf8(bytes).is_ok(),
                                        DataFormat::Protobuf => pb::BenchmarkPayload::decode(bytes.as_slice()).is_ok(),
                                    }
                                },
                                Ok(None) => false,
                                Err(_) => false,
                            }
                        }
                    };

                    if let Some(st) = op_start_time {
                        if let Some(h) = hist.as_mut() { let _ = h.record(st.elapsed().as_micros() as u64); }
                    }

                    if !op_successful { local_errors += 1; }
                    local_ops_done += 1;

                    worker_progress_counter += 1;
                    if worker_progress_counter >= PROGRESS_UPDATE_INTERVAL as u64 {
                        progress_bar_clone.inc(worker_progress_counter);
                        worker_progress_counter = 0;
                    }
                }
            }
            if worker_progress_counter > 0 {
                progress_bar_clone.inc(worker_progress_counter);
            }
            WorkerResult { histogram: hist, errors: local_errors, ops_done: local_ops_done }
        }));
    }

    let worker_results = futures::future::join_all(tasks).await;
    let total_time = start_time.elapsed();
    
    if pb_arc.position() < num_ops as u64 && num_ops > 0 {
        pb_arc.set_position(num_ops as u64);
    }
    pb_arc.finish_with_message(format!("{} {} {} Done", db_name, op_type_owned, mode_str));

    let mut final_histogram = if track_latency && !use_pipeline {
        Some(hdrhistogram::Histogram::<u64>::new(3).unwrap())
    } else { None };
    let mut total_errors = 0;

    for result in worker_results {
        match result {
            Ok(wr) => {
                if let (Some(fh), Some(wh)) = (final_histogram.as_mut(), wr.histogram.as_ref()) {
                    let _ = fh.add(wh);
                }
                total_errors += wr.errors;
            }
            Err(e) => {
                eprintln!("Worker task panicked: {}", e);
                total_errors += num_ops / concurrency.max(1);
            }
        }
    }

    if total_errors > 0 {
        println!("WARNING: {}/{} operations reported errors for {} {}", total_errors, num_ops, db_name, op_type_owned);
    }

    let successful_ops = num_ops.saturating_sub(total_errors);
    let ops_per_second = if total_time.as_secs_f64() > 0.0 {
        successful_ops as f64 / total_time.as_secs_f64()
    } else { if successful_ops > 0 { f64::INFINITY } else { 0.0 } };

    let (avg_lat, p99_lat) = if let Some(hist) = final_histogram.as_ref() {
        if hist.len() > 0 { (Some(hist.mean() as f64 / 1000.0), Some(hist.value_at_percentile(99.0) as f64 / 1000.0)) }
        else { (Some(0.0), Some(0.0)) }
    } else { (None, None) };

    Ok(BenchResult { ops_per_second, total_time, avg_latency_ms: avg_lat, p99_latency_ms: p99_lat, errors: total_errors })
}

#[tokio::main]
async fn main() -> Result<(), Box<dyn std::error::Error>> {
    let cli = Cli::parse();
    println!(
        "Configuration: Ops={}, Concurrency={}, Format={:?}, Key={}B, ValBase={}B, Pipeline={}, BatchSize={}, NoLatency={}, ZeroCopyRead={}",
        cli.num_ops, cli.concurrency, cli.format, cli.key_size, cli.value_size, cli.pipeline, cli.batch_size, cli.no_latency, cli.zero_copy_read
    );
    println!("Ensure Redis is running on {} and Valkey on {}", cli.redis_url, cli.valkey_url);

    if cli.num_ops == 0 { println!("Number of operations is 0, exiting."); return Ok(()); }
    let safe_concurrency = cli.concurrency.max(1);

    let benchmarks_to_run = match (cli.write_only, cli.read_only) {
        (true, false) => vec!["WRITE"],
        (false, true) => vec!["READ"],
        (false, false) => vec!["WRITE", "READ"],
        (true, true) => { println!("Cannot specify both --write-only and --read-only."); return Ok(()); }
    };

    let track_latency_globally = !cli.no_latency;

    let write_data = PreGeneratedData::new(cli.num_ops, cli.key_size, cli.value_size, "WRITE", cli.format);
    let read_data = PreGeneratedData { keys: Arc::clone(&write_data.keys), values: None };

    if benchmarks_to_run.contains(&"READ") {
        let pre_populate_pipeline = true;
        let pre_populate_batch_size = cli.batch_size.max(200);
        let pre_populate_track_latency = false;
        println!("\nINFO: Pre-populating Redis for READ benchmark...");
        run_benchmark("Redis-PrePop", "WRITE", &cli.redis_url, cli.num_ops, safe_concurrency, &write_data, pre_populate_pipeline, pre_populate_batch_size, pre_populate_track_latency, cli.format, false).await?;
        println!("\nINFO: Pre-populating Valkey for READ benchmark...");
        run_benchmark("Valkey-PrePop", "WRITE", &cli.valkey_url, cli.num_ops, safe_concurrency, &write_data, pre_populate_pipeline, pre_populate_batch_size, pre_populate_track_latency, cli.format, false).await?;
    }

    let mut results_table = Vec::new();
    for db_type in &["Redis", "Valkey"] {
        let url_string = if *db_type == "Redis" { &cli.redis_url } else { &cli.valkey_url };
        for op_type_str_ref in &benchmarks_to_run {
            let current_data = if *op_type_str_ref == "WRITE" { &write_data } else { &read_data };
            let actual_track_latency = track_latency_globally && !cli.pipeline;
            let zero_copy_for_this_run = if *op_type_str_ref == "READ" { cli.zero_copy_read } else { false };

            match run_benchmark(db_type, op_type_str_ref, url_string.as_str(), cli.num_ops, safe_concurrency, current_data, cli.pipeline, cli.batch_size, actual_track_latency, cli.format, zero_copy_for_this_run).await {
                Ok(result) => { results_table.push(( *db_type, *op_type_str_ref, result.ops_per_second, result.total_time.as_secs_f64(), result.avg_latency_ms, result.p99_latency_ms, result.errors )); },
                Err(e) => {
                    eprintln!("Error benchmarking {} {}: {}", db_type, op_type_str_ref, e);
                     match redis::Client::open(url_string.as_str()) {
                        Ok(client_ping) => {
                            match client_ping.get_multiplexed_async_connection().await {
                                Ok(mut conn_ping) => {
                                    match redis::cmd("PING").query_async::<String>(&mut conn_ping).await {
                                        Ok(pong) if pong == "PONG" => println!("{} is responsive (PING successful).", db_type),
                                        _ => println!("{} is NOT responsive or PING failed.", db_type),
                                    }
                                }
                                Err(ce) => println!("Could not connect to {} for PING: {}", db_type, ce),
                            }
                        }
                        Err(oe) => println!("Could not open client for {} for PING: {}", db_type, oe),
                    }
                }
            }
        }
    }

    println!("\n--- Benchmark Summary (Format: {:?}) ---", cli.format);
    let show_latency_in_table = track_latency_globally && !cli.pipeline;
    if show_latency_in_table {
        println!("{:<15} | {:<11} | {:<15} | {:<12} | {:<12} | {:<12} | {:<8}", "Database", "Op Type", "Ops/sec", "Total Time(s)", "Avg Lat(ms)", "P99 Lat(ms)", "Errors");
        println!("{:-<16}|{:-<13}|{:-<17}|{:-<14}|{:-<14}|{:-<14}|{:-<10}", "", "", "", "", "", "", "");
    } else {
        println!("{:<15} | {:<11} | {:<15} | {:<12} | {:<8}", "Database", "Op Type", "Ops/sec", "Total Time(s)", "Errors");
        println!("{:-<16}|{:-<13}|{:-<17}|{:-<14}|{:-<10}", "", "", "", "", "");
    }

    for (db, op, ops_sec, time_s, avg_lat, p99_lat, errors) in results_table.iter() {
        let op_name = if *op == "READ" && cli.zero_copy_read {
            "READ (Raw)".to_string()
        } else {
            op.to_string()
        };

        if show_latency_in_table {
            println!("{:<15} | {:<11} | {:<15.2} | {:<12.3} | {:<12.3} | {:<12.3} | {:<8}", db, op_name, ops_sec, time_s, avg_lat.unwrap_or(0.0), p99_lat.unwrap_or(0.0), errors);
        } else {
            println!("{:<15} | {:<11} | {:<15.2} | {:<12.3} | {:<8}", db, op_name, ops_sec, time_s, errors);
        }
    }

    Ok(())
}